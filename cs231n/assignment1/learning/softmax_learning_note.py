"""
Cross entropy beweet P,Q, P is the truth distribution, Q is the estimated distribution: H(P,Q) = -PLog(Q)
If the log base is 2, then it is a bit, if log base is natural number e, then it is nats
Entropy of P: H(P) = -log(P)
KL Divergence: KL(P|Q)
H(P,Q) = H(P) + KL(P|Q)



"""